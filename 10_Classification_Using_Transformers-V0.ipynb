{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452e87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFRobertaModel, DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import AutoConfig\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fb9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3 = pd.read_csv(\"data_v2/Aug_BackTranslation.csv\")\n",
    "\n",
    "from_french = train_data_3[[\"from_french\", \"sentiment\"]].dropna()\n",
    "from_french.rename(columns={\"from_french\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_italian = train_data_3[[\"from_italian\", \"sentiment\"]].dropna()\n",
    "from_italian.rename(columns={\"from_italian\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_german = train_data_3[[\"from_german\", \"sentiment\"]].dropna()\n",
    "from_german.rename(columns={\"from_german\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_chinese = train_data_3[[\"from_chinese\", \"sentiment\"]].dropna()\n",
    "from_chinese.rename(columns={\"from_chinese\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "train_data_3 = pd.concat([from_french, from_italian, from_german, from_chinese], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_3 = pd.read_csv(\"data_v2/Aug_Val_BackTranslation.csv\")\n",
    "\n",
    "from_french = val_data_3[[\"from_french\", \"sentiment\"]].dropna()\n",
    "from_french.rename(columns={\"from_french\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_italian = val_data_3[[\"from_italian\", \"sentiment\"]].dropna()\n",
    "from_italian.rename(columns={\"from_italian\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_german = val_data_3[[\"from_german\", \"sentiment\"]].dropna()\n",
    "from_german.rename(columns={\"from_german\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_chinese = val_data_3[[\"from_chinese\", \"sentiment\"]].dropna()\n",
    "from_chinese.rename(columns={\"from_chinese\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "val_data_3 = pd.concat([from_french, from_italian, from_german, from_chinese], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff5320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv(\"data_v2/train_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "train_data_2 = pd.read_csv(\"data_v2/Aug_RandomInsertion.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "train_data = pd.concat([train_data_1, train_data_2, train_data_3], ignore_index=True)\n",
    "\n",
    "val_data_1 = pd.read_csv(\"data_v2/validation_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data_2 = pd.read_csv(\"data_v2/Aug_Val_RandomInsertion.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data = pd.concat([val_data_1, val_data_2, val_data_3], ignore_index=True)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"data_v2/test_data.csv\", usecols=[\"reviewText\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55898128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e7bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6935a939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5748, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b3ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    1\n",
       "sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ed47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "train_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)\n",
    "\n",
    "val_data.dropna(inplace=True)\n",
    "val_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "test_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8749022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5747, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87e5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b25c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20675b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ds = Dataset.from_pandas(train_data)\n",
    "val_data_ds = Dataset.from_pandas(val_data)\n",
    "test_data_ds = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938b7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdec297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ac666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, bert_encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = bert_encoder\n",
    "        self.classifier_1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.4)\n",
    "        self.classifier_2 = tf.keras.layers.Dense(32, activation=\"tanh\")\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.4)\n",
    "        self.classifier_3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.encoder(inputs, training=training)[\"last_hidden_state\"][:,0]\n",
    "        x = self.classifier_1(x)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.classifier_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        x = self.classifier_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc36c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x17b5389d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c7324bb0fc47338fdda569c467a7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048cfcd28dfb4faabb59d45e52abbb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f176769c5bb4e9093b486ecd63b2648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:07:01.634280: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-19 22:07:01.634481: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "train_datasets = train_data_ds.map(tokenize_function, batched=True)\n",
    "val_datasets = val_data_ds.map(tokenize_function, batched=True)\n",
    "test_datasets = test_data_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "tf_train_dataset = train_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "tf_train_dataset = tf_train_dataset.cache()\n",
    "\n",
    "tf_validation_dataset = val_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")\n",
    "tf_validation_dataset = tf_validation_dataset.cache()\n",
    "\n",
    "tf_test_dataset = test_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dbc44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_encoder = TFRobertaModel.from_pretrained(checkpoint, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a451b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(bert_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9dfe655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/train_data.csv\", usecols=[\"sentiment\"])\n",
    "# total = len(data)\n",
    "# data = pd.read_csv(\"data/train_data.csv\", usecols=[\"sentiment\"])\n",
    "# classes = data[\"sentiment\"].value_counts().to_dict()\n",
    "# total = len(data)\n",
    "# weight_for_0 = (1 / classes[0]) * (total / 2.0)\n",
    "# weight_for_1 = (1 / classes[1]) * (total / 2.0)\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c6d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'classifier_checkpoint_v0/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model_checkpoint_earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4388e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9afbbf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:07:05.767633: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:07:10.417312: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - ETA: 0s - loss: 0.7431 - accuracy: 0.4778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:23:47.766957: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 1093s 6s/step - loss: 0.7431 - accuracy: 0.4778 - val_loss: 0.6898 - val_accuracy: 0.4563\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 1432s 8s/step - loss: 0.7041 - accuracy: 0.4856 - val_loss: 0.6903 - val_accuracy: 0.4563\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 1650s 9s/step - loss: 0.6968 - accuracy: 0.4801 - val_loss: 0.6894 - val_accuracy: 0.4563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2971d8610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    callbacks=[model_checkpoint_callback, model_checkpoint_earlyStopping],\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826909e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 52s 2s/step - loss: 0.6379 - accuracy: 0.1048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6378714442253113, 0.10478360205888748]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e27bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "838f2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a480fc7fc00c46dcbbfa766527a143bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02772f0edf143aead892e94f09a7ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_for_pred = pd.read_csv(\"data_v2/train_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data_for_pred = pd.read_csv(\"data_v2/validation_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "\n",
    "train_data_for_pred = pd.concat([train_data_for_pred, val_data_for_pred], ignore_index=True)\n",
    "\n",
    "test_data_for_pred = pd.read_csv(\"data_v2/test_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "\n",
    "train_data_for_pred.dropna(inplace=True)\n",
    "train_data_for_pred.rename(columns={\"reviewText\": \"text\", \n",
    "                                    \"sentiment\": \"labels\"}, \n",
    "                           inplace=True)\n",
    "\n",
    "\n",
    "test_data_for_pred.dropna(inplace=True)\n",
    "test_data_for_pred.rename(columns={\"reviewText\": \"text\", \n",
    "                                   \"sentiment\": \"labels\"}, \n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "train_data_for_pred_ds = Dataset.from_pandas(train_data_for_pred)\n",
    "test_data_for_pred_ds = Dataset.from_pandas(test_data_for_pred)\n",
    "\n",
    "train_datasets_for_pred = train_data_for_pred_ds.map(tokenize_function, batched=True)\n",
    "test_datasets_for_pred = test_data_for_pred_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "\n",
    "tf_train_dataset_for_pred = train_datasets_for_pred.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "tf_train_dataset_for_pred = tf_train_dataset_for_pred.cache()\n",
    "\n",
    "\n",
    "tf_test_dataset_for_pred = test_datasets_for_pred.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")\n",
    "tf_test_dataset_for_pred = tf_test_dataset_for_pred.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fbd9bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 23:46:12.276522: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "train_predict = model.predict(tf_train_dataset_for_pred)\n",
    "test_predict = model.predict(tf_test_dataset_for_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "298bc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_logits = train_predict.ravel()\n",
    "test_predict_logits = test_predict.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87d4bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d545f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data_for_pred[\"labels\"].values\n",
    "test_labels = test_data_for_pred[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed447aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.146658, Training F-Score=0.944\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(train_labels, train_predict_logits)\n",
    "\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall+0.000000001)\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, Training F-Score=%.3f' % (thresholds[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f05ec733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89361702, 0.89359007, 0.89378961, 0.89418929, 0.89410868,\n",
       "        0.89433384, 0.89442589, 0.89454545, 0.8939314 , 0.89449291,\n",
       "        0.89420085, 0.8967587 , 0.89777491, 0.89618266, 0.8927571 ,\n",
       "        0.89343881, 0.88957055, 0.89280677, 0.89620019, 0.88578372,\n",
       "        0.89695946, 0.89356436, 0.89411765, 0.8972973 , 0.90909091,\n",
       "        0.89285714, 0.84      , 0.75      , 1.        , 1.        ]),\n",
       " array([1.00000000e+00, 9.99716553e-01, 9.99433107e-01, 9.98866213e-01,\n",
       "        9.98015873e-01, 9.93197279e-01, 9.86961451e-01, 9.76190476e-01,\n",
       "        9.60317460e-01, 9.29988662e-01, 8.95975057e-01, 8.46938776e-01,\n",
       "        7.89115646e-01, 7.12018141e-01, 6.32369615e-01, 5.44217687e-01,\n",
       "        4.52097506e-01, 3.58843537e-01, 2.74092971e-01, 2.06632653e-01,\n",
       "        1.50510204e-01, 1.02324263e-01, 6.46258503e-02, 4.70521542e-02,\n",
       "        2.55102041e-02, 1.41723356e-02, 5.95238095e-03, 2.55102041e-03,\n",
       "        5.66893424e-04, 0.00000000e+00]),\n",
       " array([0.14665829, 0.14665835, 0.14665836, 0.14665838, 0.14665839,\n",
       "        0.1466584 , 0.14665842, 0.14665844, 0.14665845, 0.14665847,\n",
       "        0.14665848, 0.1466585 , 0.14665851, 0.14665852, 0.14665854,\n",
       "        0.14665855, 0.14665857, 0.14665858, 0.1466586 , 0.14665861,\n",
       "        0.14665863, 0.14665864, 0.14665866, 0.14665867, 0.14665869,\n",
       "        0.1466587 , 0.14665872, 0.14665873, 0.14665876], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb994b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d64f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.64      0.18       420\n",
      "           1       0.89      0.36      0.51      3528\n",
      "\n",
      "    accuracy                           0.39      3948\n",
      "   macro avg       0.50      0.50      0.35      3948\n",
      "weighted avg       0.81      0.39      0.48      3948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predict_labels = [int(x>=0.14665857) for x in train_predict_logits]\n",
    "\n",
    "print(classification_report(train_labels, train_predict_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0eb20b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.65      0.18        46\n",
      "           1       0.89      0.35      0.50       393\n",
      "\n",
      "    accuracy                           0.38       439\n",
      "   macro avg       0.50      0.50      0.34       439\n",
      "weighted avg       0.81      0.38      0.47       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_labels = [int(x>=0.14665857) for x in test_predict_logits]\n",
    "\n",
    "print(classification_report(test_labels, test_predict_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ce14f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.save_pretrained(\"roberta-finetuned-V1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c674217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y, y_pred):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for actual_value, predicted_value in zip(y, y_pred):\n",
    "        if predicted_value == actual_value:  # t?\n",
    "            if predicted_value:  # tp\n",
    "                tp += 1\n",
    "            else:  # tn\n",
    "                tn += 1\n",
    "        else:  # f?\n",
    "            if predicted_value:  # fp\n",
    "                fp += 1\n",
    "            else:  # fn\n",
    "                fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print('----------------------------------')\n",
    "    print('                 Actual Value')\n",
    "    print('----------------------------------')\n",
    "    print(f'            Positive    Negative')\n",
    "    print(f'Positive    {tp:^8}    {fp:^8}')\n",
    "    print(f'Negative    {fn:^8}    {tn:^8}')\n",
    "    print('----------------------------------')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6341a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "                 Actual Value\n",
      "----------------------------------\n",
      "            Positive    Negative\n",
      "Positive      136          16   \n",
      "Negative      257          30   \n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49908256880733937"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_labels, test_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f36cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
