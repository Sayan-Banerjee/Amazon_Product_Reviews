{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452e87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFAutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import AutoConfig\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import RobertaConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fb9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3 = pd.read_csv(\"data_v2/Aug_BackTranslation.csv\")\n",
    "\n",
    "from_french = train_data_3[[\"from_french\", \"sentiment\"]].dropna()\n",
    "from_french.rename(columns={\"from_french\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_italian = train_data_3[[\"from_italian\", \"sentiment\"]].dropna()\n",
    "from_italian.rename(columns={\"from_italian\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_german = train_data_3[[\"from_german\", \"sentiment\"]].dropna()\n",
    "from_german.rename(columns={\"from_german\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_chinese = train_data_3[[\"from_chinese\", \"sentiment\"]].dropna()\n",
    "from_chinese.rename(columns={\"from_chinese\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "train_data_3 = pd.concat([from_french, from_italian, from_german, from_chinese], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348e6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_3 = pd.read_csv(\"data_v2/Aug_Val_BackTranslation.csv\")\n",
    "\n",
    "from_french = val_data_3[[\"from_french\", \"sentiment\"]].dropna()\n",
    "from_french.rename(columns={\"from_french\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_italian = val_data_3[[\"from_italian\", \"sentiment\"]].dropna()\n",
    "from_italian.rename(columns={\"from_italian\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_german = val_data_3[[\"from_german\", \"sentiment\"]].dropna()\n",
    "from_german.rename(columns={\"from_german\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "from_chinese = val_data_3[[\"from_chinese\", \"sentiment\"]].dropna()\n",
    "from_chinese.rename(columns={\"from_chinese\": \"reviewText\"}, inplace=True)\n",
    "\n",
    "val_data_3 = pd.concat([from_french, from_italian, from_german, from_chinese], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff5320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv(\"data_v2/train_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "train_data_2 = pd.read_csv(\"data_v2/Aug_RandomInsertion.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "train_data = pd.concat([train_data_1, train_data_2, train_data_3], ignore_index=True)\n",
    "\n",
    "val_data_1 = pd.read_csv(\"data_v2/validation_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data_2 = pd.read_csv(\"data_v2/Aug_Val_RandomInsertion.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data = pd.concat([val_data_1, val_data_2, val_data_3], ignore_index=True)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"data_v2/test_data.csv\", usecols=[\"reviewText\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55898128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e7bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6935a939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5748, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b3ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    1\n",
       "sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ed47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "train_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)\n",
    "\n",
    "val_data.dropna(inplace=True)\n",
    "val_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "test_data.rename(columns={\"reviewText\": \"text\", \n",
    "                           \"sentiment\": \"labels\"}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8749022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5747, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87e5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b25c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20675b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ds = Dataset.from_pandas(train_data)\n",
    "val_data_ds = Dataset.from_pandas(val_data)\n",
    "test_data_ds = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938b7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fdec297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac666b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc36c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x28e4e6c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7281751024a02ab272ea85147f288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd404aa5e71841659454605de7b65109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c40b572e324d9a9cd95c305cda3f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "train_datasets = train_data_ds.map(tokenize_function, batched=True)\n",
    "val_datasets = val_data_ds.map(tokenize_function, batched=True)\n",
    "test_datasets = test_data_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "tf_train_dataset = train_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "tf_train_dataset = tf_train_dataset.cache()\n",
    "\n",
    "tf_validation_dataset = val_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")\n",
    "tf_validation_dataset = tf_validation_dataset.cache()\n",
    "\n",
    "tf_test_dataset = test_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dbc44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_encoder = TFRobertaModel.from_pretrained(checkpoint, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a451b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Classifier(bert_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9dfe655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/train_data.csv\", usecols=[\"sentiment\"])\n",
    "# total = len(data)\n",
    "# data = pd.read_csv(\"data/train_data.csv\", usecols=[\"sentiment\"])\n",
    "# classes = data[\"sentiment\"].value_counts().to_dict()\n",
    "# total = len(data)\n",
    "# weight_for_0 = (1 / classes[0]) * (total / 2.0)\n",
    "# weight_for_1 = (1 / classes[1]) * (total / 2.0)\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c6d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'classifier_checkpoint_v2/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model_checkpoint_earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4388e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9afbbf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:56:20.566235: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:56:26.120824: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.5147 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:36:20.828702: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 2509s 14s/step - loss: 0.7116 - accuracy: 0.5147 - val_loss: 0.6894 - val_accuracy: 0.5437\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 2701s 15s/step - loss: 0.6955 - accuracy: 0.5291 - val_loss: 0.6894 - val_accuracy: 0.5437\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 1640s 9s/step - loss: 0.6943 - accuracy: 0.5338 - val_loss: 0.6923 - val_accuracy: 0.5437\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 1618s 9s/step - loss: 0.6931 - accuracy: 0.5351 - val_loss: 0.6901 - val_accuracy: 0.5437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28fb9e820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    callbacks=[model_checkpoint_callback, model_checkpoint_earlyStopping],\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc8a23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 53s 2s/step - loss: 0.6014 - accuracy: 0.8952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6014251708984375, 0.8952164649963379]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd94b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eb68a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc91ce7a3f714bbcb605f42a56f97305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61dd351073b4902a4873371c34c6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_for_pred = pd.read_csv(\"data_v2/train_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "val_data_for_pred = pd.read_csv(\"data_v2/validation_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "\n",
    "train_data_for_pred = pd.concat([train_data_for_pred, val_data_for_pred], ignore_index=True)\n",
    "\n",
    "test_data_for_pred = pd.read_csv(\"data_v2/test_data.csv\", usecols=[\"reviewText\", \"sentiment\"])\n",
    "\n",
    "train_data_for_pred.dropna(inplace=True)\n",
    "train_data_for_pred.rename(columns={\"reviewText\": \"text\", \n",
    "                                    \"sentiment\": \"labels\"}, \n",
    "                           inplace=True)\n",
    "\n",
    "\n",
    "test_data_for_pred.dropna(inplace=True)\n",
    "test_data_for_pred.rename(columns={\"reviewText\": \"text\", \n",
    "                                   \"sentiment\": \"labels\"}, \n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "train_data_for_pred_ds = Dataset.from_pandas(train_data_for_pred)\n",
    "test_data_for_pred_ds = Dataset.from_pandas(test_data_for_pred)\n",
    "\n",
    "train_datasets_for_pred = train_data_for_pred_ds.map(tokenize_function, batched=True)\n",
    "test_datasets_for_pred = test_data_for_pred_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "\n",
    "tf_train_dataset_for_pred = train_datasets_for_pred.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "tf_train_dataset_for_pred = tf_train_dataset_for_pred.cache()\n",
    "\n",
    "\n",
    "tf_test_dataset_for_pred = test_datasets_for_pred.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=16,\n",
    ")\n",
    "tf_test_dataset_for_pred = tf_test_dataset_for_pred.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1aaf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(tf_train_dataset_for_pred)\n",
    "test_predict = model.predict(tf_test_dataset_for_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dbc379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_logits = train_predict.logits[:,1]\n",
    "test_predict_logits = test_predict.logits[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "217c8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data_for_pred[\"labels\"].values\n",
    "test_labels = test_data_for_pred[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f0678e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.185000, Training F-Score=0.944\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(train_labels, train_predict_logits)\n",
    "\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall+0.000000001)\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, Training F-Score=%.3f' % (thresholds[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2161f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89361702, 0.89485861, 0.89538542, 0.89742605, 0.89785228,\n",
       "        0.88518519, 0.87037037, 1.        , 1.        , 1.        ]),\n",
       " array([1.00000000e+00, 9.86678005e-01, 9.67970522e-01, 6.62131519e-01,\n",
       "        4.85827664e-01, 6.77437642e-02, 2.66439909e-02, 8.50340136e-04,\n",
       "        2.83446712e-04, 0.00000000e+00]),\n",
       " array([0.18499951, 0.18499953, 0.18499954, 0.18499956, 0.18499957,\n",
       "        0.18499959, 0.1849996 , 0.18499961, 0.18499963], dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bec4337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.65      0.20        46\n",
      "           1       0.91      0.43      0.59       393\n",
      "\n",
      "    accuracy                           0.46       439\n",
      "   macro avg       0.52      0.54      0.39       439\n",
      "weighted avg       0.83      0.46      0.55       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_labels = [int(x>=0.18499956) for x in test_predict_logits]\n",
    "\n",
    "print(classification_report(test_labels, test_predict_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b805e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"roberta-finetuned-V3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f0cc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y, y_pred):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for actual_value, predicted_value in zip(y, y_pred):\n",
    "        if predicted_value == actual_value:  # t?\n",
    "            if predicted_value:  # tp\n",
    "                tp += 1\n",
    "            else:  # tn\n",
    "                tn += 1\n",
    "        else:  # f?\n",
    "            if predicted_value:  # fp\n",
    "                fp += 1\n",
    "            else:  # fn\n",
    "                fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print('----------------------------------')\n",
    "    print('                 Actual Value')\n",
    "    print('----------------------------------')\n",
    "    print(f'            Positive    Negative')\n",
    "    print(f'Positive    {tp:^8}    {fp:^8}')\n",
    "    print(f'Negative    {fn:^8}    {tn:^8}')\n",
    "    print('----------------------------------')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f05ec733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "                 Actual Value\n",
      "----------------------------------\n",
      "            Positive    Negative\n",
      "Positive      170          16   \n",
      "Negative      223          30   \n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5872193436960277"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_labels, test_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221b11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
